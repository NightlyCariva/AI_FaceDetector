import streamlit as st
import cv2
import tempfile
import os
from datetime import datetime
import io
import sys
from contextlib import redirect_stdout, redirect_stderr
from face_detector import FaceDetector

def run_mode1():
    """Interface du Mode 1: Upload Vid√©o"""
    
    st.title("Mode 1: Analyse de Vid√©o Upload√©e")
    st.markdown("---")
    
    st.header("Fonctionnement")
    st.info("""
    Ce mode permet d'analyser des vid√©os pr√©-enregistr√©es. Uploadez votre fichier vid√©o, 
    configurez les param√®tres d'analyse, puis lancez le traitement. Vous pourrez ensuite 
    t√©l√©charger les r√©sultats sous forme de CSV et la vid√©o annot√©e.
    """)
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.header("Param√®tres")
        
        temperature = st.slider("Temp√©rature", 0.0, 1.0, 0.5, 0.1)
        
        st.subheader("Options d'analyse")
        analyze_age = st.checkbox("Age", value=True)
        analyze_gender = st.checkbox("Genre", value=True)
        analyze_emotion = st.checkbox("Emotion", value=True)
        analyze_ethnicity = st.checkbox("Ethnie", value=True)
        
        use_gpu = st.checkbox("Acc√©l√©ration mat√©riel (GPU)", value=False)
        
        st.subheader("Param√®tres de tracking")
        detection_interval = st.slider(
            "Intervalle de d√©tection (frames)", 
            min_value=10, 
            max_value=60, 
            value=30, 
            step=5,
            help="Fr√©quence de recherche de nouveaux visages (plus √©lev√© = plus rapide)"
        )
        
        st.header("Upload Vid√©o")
        uploaded_file = st.file_uploader(
            "Choisissez un fichier vid√©o",
            type=['mp4', 'avi', 'mov', 'mkv'],
            help="Formats support√©s: MP4, AVI, MOV, MKV"
        )
        
        if uploaded_file is not None:
            file_details = {
                "Nom": uploaded_file.name,
                "Taille": f"{uploaded_file.size / (1024*1024):.2f} MB",
                "Type": uploaded_file.type
            }
            
            st.write("**D√©tails du fichier:**")
            for key, value in file_details.items():
                st.write(f"- {key}: {value}")
            
            if st.button("Analyser la Vid√©o", type="primary"):
                process_video(
                    uploaded_file, temperature, analyze_age, analyze_gender, 
                    analyze_emotion, analyze_ethnicity, use_gpu, detection_interval
                )
    
    with col2:
        st.header("R√©sultats")
        
        if 'video_results' in st.session_state:
            if st.button("üóëÔ∏è Effacer les r√©sultats", key="clear_results"):
                del st.session_state.video_results
                if 'console_output' in st.session_state:
                    del st.session_state.console_output
                st.rerun()
            
            display_results()
        else:
            st.info("Uploadez une vid√©o et lancez l'analyse pour voir les r√©sultats ici.")
    
    st.markdown("---")
    st.header("Console")
    
    if 'console_output' in st.session_state:
        st.text_area(
            "Logs d'ex√©cution",
            value=st.session_state.console_output,
            height=200,
            disabled=True
        )
    else:
        st.text_area(
            "Logs d'ex√©cution",
            value="En attente de traitement...",
            height=200,
            disabled=True
        )

def process_video(uploaded_file, temperature, analyze_age, analyze_gender, 
                 analyze_emotion, analyze_ethnicity, use_gpu, detection_interval):
    """Traite la vid√©o upload√©e"""
    
    console_output = io.StringIO()
    
    try:
        with redirect_stdout(console_output), redirect_stderr(console_output):
            print(f"[{datetime.now().strftime('%H:%M:%S')}] D√©but du traitement")
            print(f"Fichier: {uploaded_file.name}")
            print(f"Param√®tres: Age={analyze_age}, Genre={analyze_gender}, Emotion={analyze_emotion}, Ethnie={analyze_ethnicity}")
            print(f"GPU: {use_gpu}")
            
            with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
                tmp_file.write(uploaded_file.read())
                input_path = tmp_file.name
            
            print(f"Fichier temporaire cr√©√©: {input_path}")
            
            detector = FaceDetector(use_gpu=use_gpu)
            detector.detection_interval = detection_interval
            print("D√©tecteur initialis√©")
            print(f"Intervalle de d√©tection configur√©: {detection_interval} frames")
            
            cap = cv2.VideoCapture(input_path)
            if not cap.isOpened():
                raise Exception("Impossible d'ouvrir la vid√©o")
            
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            print(f"Vid√©o: {total_frames} frames, {fps} FPS, {width}x{height}")
            
            output_path = tempfile.mktemp(suffix='_analyzed.mp4')
            fourcc = cv2.VideoWriter.fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            
            print("D√©but de l'analyse avec syst√®me de tracking...")
            print(f"D√©tection de nouveaux visages toutes les {detector.detection_interval} frames")
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            all_detections = []
            frame_count = 0
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                timestamp = f"{int(frame_count // fps // 3600):02d}:{int((frame_count // fps) % 3600 // 60):02d}:{int(frame_count // fps % 60):02d}"
                
                detections = detector.process_frame_with_tracking(
                    frame, frame_count, timestamp,
                    analyze_age, analyze_gender, analyze_emotion, analyze_ethnicity
                )
                
                if detections:
                    all_detections.extend(detections)
                    detector.detections.extend(detections)
                    
                    if frame_count % detector.detection_interval == 0:
                        print(f"Frame {frame_count}: {len(detections)} visage(s) track√©(s)")
                
                annotated_frame = detector.draw_annotations(
                    frame, detections,
                    analyze_age, analyze_gender, analyze_emotion, analyze_ethnicity
                )
                
                out.write(annotated_frame)
                
                frame_count += 1
                
                progress = (frame_count / total_frames)
                progress_bar.progress(progress)
                status_text.text(f"Traitement: {frame_count}/{total_frames} frames")
            
            cap.release()
            out.release()
            
            print(f"Analyse termin√©e. {len(all_detections)} d√©tections au total")
            
            st.session_state.video_results = {
                'detections': all_detections,
                'detector': detector,
                'output_video_path': output_path,
                'total_frames': total_frames,
                'fps': fps,
                'processing_completed': True
            }
            
            print("R√©sultats sauvegard√©s")
            print("=== TRAITEMENT TERMIN√â ===")
            
            st.success("‚úÖ Analyse termin√©e avec succ√®s ! Consultez les r√©sultats ci-dessous.")
            
            os.unlink(input_path)
            
    except Exception as e:
        print(f"ERREUR: {str(e)}")
        st.error(f"Erreur lors du traitement: {str(e)}")
    
    finally:
        st.session_state.console_output = console_output.getvalue()

def display_results():
    """Affiche les r√©sultats de l'analyse"""
    results = st.session_state.video_results
    detections = results['detections']
    detector = results['detector']
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Frames analys√©es", results['total_frames'])
    
    with col2:
        st.metric("D√©tections totales", len(detections))
    
    with col3:
        unique_faces = len(set(d['face_id'] for d in detections))
        st.metric("Visages uniques", unique_faces)
    
    with col4:
        if 'processing_completed' in results:
            st.metric("Statut", "‚úÖ Termin√©")
        else:
            st.metric("Statut", "üîÑ En cours")
    
    if detections:
        df = detector.get_detections_dataframe()
        
        display_df = df.drop('bbox', axis=1, errors='ignore')
        st.dataframe(display_df, use_container_width=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            csv_data = display_df.to_csv(index=False, sep=';')
            st.download_button(
                label="T√©l√©charger CSV",
                data=csv_data,
                file_name=f"detections_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
        
        with col2:
            if os.path.exists(results['output_video_path']):
                with open(results['output_video_path'], 'rb') as f:
                    st.download_button(
                        label="T√©l√©charger Vid√©o",
                        data=f.read(),
                        file_name=f"video_analyzed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4",
                        mime="video/mp4"
                    )
    else:
        st.warning("Aucun visage d√©tect√© dans la vid√©o.") 